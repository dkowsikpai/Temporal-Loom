# -*- coding: utf-8 -*-
"""Seq2Seq_Himanshu.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ej9xfnsBIoMuLJgepZOcpXcJ5TVncmkl
"""

import torch
import random
import re
import pandas as pd
import numpy as np
import os
import argparse
from transformers import AutoTokenizer
from datasets import load_dataset, Dataset, DatasetDict
from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoModelForCausalLM
import evaluate
from tqdm import tqdm
    

parser = argparse.ArgumentParser()
parser.add_argument('--train', type=str, required=True)
parser.add_argument('--val', type=str, required=True)
# parser.add_argument('--year', type=str, required=True, default="2010")
parser.add_argument('--model', type=str, default="t5-small")
parser.add_argument('--sample', type=int, default=10)
parser.add_argument('--test-model', type=bool, default=False)
parser.add_argument('--test-model-path', type=str, default="t5-small")
parser.add_argument('--cuda', type=str, default=0)
pargs = parser.parse_args()

# import wandb
os.environ["CUDA_VISIBLE_DEVICES"] = pargs.cuda
# os.environ["WANDB_PROJECT"]="generation_bot"

# save your trained model checkpoint to wandb
# os.environ["WANDB_LOG_MODEL"]="true"
def seed_everything(seed):
    import random, os
    import numpy as np
    import torch
    
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True

seed_everything(1)
# Load the dataset:
data_path = pargs.train
model_checkpoint = pargs.model
if pargs.test_model:
    model_checkpoint = pargs.test_model_path

if model_checkpoint in ["gpt2", "gpt2-xl", "t5-base", "t5-large", "t5-3b", "t5-11b"]:
    prefix = "answer: "
else:
    prefix = ""
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
if model_checkpoint in ["gpt2", "gpt2-xl"]:
    tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(model_checkpoint)
else:
    model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)

# tokenizer.save_pretrained(f"../temporal/results/pretrained-{model_checkpoint}")
# model.save_pretrained(f"../temporal/results/pretrained-{model_checkpoint}")
# print("here")
# exit()


reviews = pd.read_csv(data_path)
val_reviews = pd.read_csv(pargs.val)
print(reviews)
print(reviews.head())
reviews.info()

# Get Token length of each review
def get_token_length(review):
    # print(review)
    return tokenizer(review)['input_ids'].__len__()


# Take the average length of the reviews:
q_tokens_len = [get_token_length(review) for review in reviews['question']]
sum_all_tokens = sum(q_tokens_len)
avg_length_ques = sum_all_tokens / len(reviews['question'])
a_tokens_len = [get_token_length(review) for review in reviews['answer']]
sum_all_tokens = sum(a_tokens_len)
avg_length_ans = sum_all_tokens / len(reviews['answer'])
print("Avg length of question: ", avg_length_ques)
print("Avg length of answer: ", avg_length_ans)
print("Max length of question: ", max(q_tokens_len))
print("Max length of answer: ", max(a_tokens_len))

max_input_length = max(q_tokens_len) # max length of the input text
max_target_length = max(a_tokens_len) # max length of the target text

# raw_datasets = load_dataset("xsum")
print("Evaluation metric loaded")
metric = evaluate.load('rouge', "bleu")


ds_reviews = Dataset.from_pandas(reviews)
val_reviews = Dataset.from_pandas(val_reviews)

# train_testvalid = ds_reviews.train_test_split(shuffle = True, seed = 1, test_size=0.2)
# # Split the 10% test + valid in half test, half valid
# test_valid = train_testvalid['test'].train_test_split(shuffle = True, seed = 1, test_size=0.5)
# gather everyone if you want to have a single DatasetDict
train_test_valid_dataset = DatasetDict({
    'train': ds_reviews,
    'test': val_reviews,
    'validation': val_reviews})



raw_datasets = train_test_valid_dataset


def preprocess_function(examples):
    inputs = [prefix + doc for doc in examples["question"]]
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)

    # Setup the tokenizer for targets
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples["answer"], max_length=max_target_length, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

print("Preprocessing function running")
preprocess_function(raw_datasets['train'][:2])

tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)


print("Setting up training arguments")
batch_size = 8
model_name = model_checkpoint.split("/")[-1]
args = Seq2SeqTrainingArguments(
    f"./logs/{model_name}-finetuned2",
    evaluation_strategy = "epoch",
    logging_steps=100, 
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    save_total_limit=1,
    num_train_epochs=10,
    save_strategy="steps",
    eval_steps = 100,
    do_eval=True,
    do_train=True,
    predict_with_generate=True,
    seed=1,
    report_to = "tensorboard"
)
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=tokenizer.pad_token_id)
import nltk
import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    
    # Rouge expects a newline after each sentence
    decoded_preds = ["\n".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]
    
    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    # Extract a few results
    # result = {key: value for key, value in result.items()}
    
    # Add mean generated length
    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]
    result["gen_len"] = np.mean(prediction_lens)
    
    return {k: round(v, 4) for k, v in result.items()}

print("Setting up trainer")
trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

if not pargs.test_model:
    print("Training")
    trainer.train()

# Generate some predictions
print("Generating predictions")
samples = pargs.sample

data_to_csv = []

our_metrics = {
    "accuracy": [],
    "exact_odering": [],
    "relaxed_ordering": [],
}

from metrics import accuracy, exact_ordering, relaxed_ordering

for i in tqdm(range(samples)):
    # idx = random.randint(0, len(ds_reviews))
    idx = i
    question = prefix + ds_reviews['question'][idx]
    answer = ds_reviews['answer'][idx]
    input_dict = tokenizer(question, return_tensors="pt")
    input_ids = input_dict["input_ids"].to("cuda")
    pred_ids = trainer.model.generate(input_ids)
    pred_answer = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)[0]

    acc = accuracy(pred_answer, answer)
    eo = exact_ordering(pred_answer, answer)
    ro = relaxed_ordering(pred_answer, answer)

    our_metrics["accuracy"].append(acc)
    our_metrics["exact_odering"].append(eo)
    our_metrics["relaxed_ordering"].append(ro)

    data_to_csv.append([question, answer, pred_answer, acc, eo, ro])

    # print("Question: ", question)
    # print("Answer: ", answer)
    # print("Predicted answer: ", pred_answer)
    # print("\n")

df = pd.DataFrame(data_to_csv, columns=["Question", "Answer", "Predicted Answer", "Accuracy", "Exact Ordering", "Relaxed Ordering"])
df.to_csv(f"./logs/{model_name}-finetuned2/results.csv")
print("Saved to", f"./logs/{model_name}-finetuned2/results.csv")